
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Diffusion-Guided 3D Policy Learning for Generalizable Robotic Manipulation">
  <meta name="keywords" content="Neural Rendering, Foundation Models, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NeRFuser</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">Diffusion-Guided 3D Policy Learning for Generalizable Robotic Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous
            </span>
          </div>



            </div>
          </div>
<!--           <br>
          <br> -->


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <img src="figures/main.png" alt="Description of the image">
      <h2 class="subtitle has-text-centered">
      </br>
        Leveraging semantic information from massive 2D images and geometric information from 
        3D point clouds, we present Semantic-Geometric Representation (SGR) that enables the robots 
        to solve a range of simulated and real-world manipulation tasks.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-block">
          <video poster="" id="block" autoplay muted loop width="50%">
            <source src="/videos/Stack_block.mp4"
                    type="video/mp4">
          </video>
        </div>
                <div class="item item-shape">
          <video poster="" id="shape" autoplay muted loop width="50%">
            <source src="/videos/sweep_dustpan.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-block-bowl">
          <video poster="" id="block-bowl" autoplay muted loop width="50%">
            <source src="media/videos/hit_ball.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-block">
          <video poster="" id="block" autoplay muted loop width="50%">
            <source src="media/videos/put_in_bin.mp4"
                    type="video/mp4">
          </video>
        </div>

        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
		This paper presents NeRFuser, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. Leveraging feature distilled 
		from foundation models such as Stable Diffusion, we learn a generalizable 3D 
		representation by pre-training a 3D voxel encoder via neural rendering. 
		Consequently, it allows various application to challenging robotic tasks requiring rich 
		3D semantics and accurate geometry. Furthermore, we introduce a novel approach 
		utilizing diffusion training to optimize features fused with vision and language 
		embeddings. By concentrating on the optimization of fused features and predict
		actions with an additional policy network, we not only eliminate the need for
		time-consuming denoising process during action inference but also align features
		with the multi-modality inherent in multi-task demonstrations, thereby enhancing
		robustness and generalizability. NeRFuser significantly surpasses SOTA NeRF-
		based multi-task manipulation approaches, achieving a 1.37x increase in success rate.
        </div>
          <br>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">

                <h2 class="title is-3">Video</h2>
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/5IZkbUFB2cM" width="1028" height="578" allow="autoplay"></iframe>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
    <!-- Animation. -->
      <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <!--<div class="rows is-centered ">
      <div class="row is-full-width">-->
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        </br>
        <img src="media/figures/method.jpg" class="interpolation-image"
         alt="Interpolate start reference image." />
        </br>
        </br>
          <p>
The pipeline of policy adaptation from foundation model feedback (PAFF). When we adapt a trained policy to a new task,
              we first let the robot <b>play</b>, that is, the policy continuously predicts and performs actions given a series of randomly generated language instructions.
              We record these demonstrations including the visual observations and the model's actions. After that, we let the model <b>relabel</b>, that is,
              the vision-language foundation model relabels the demonstrations by retrieving the language instructions given the recorded visual observations.
              We then fine-tune the policy with the accurate paired observations and instructions, and the corresponding actions, which are collected in an automatic way.
          </p>
        <!--/ Re-rendering. -->
            </div>
            </div>
          </div>
        </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Sim-to-real Transfer</h2>
        <p>
        We train a policy on simulation data and adapt it to the real world.
          </p>
          </br>
        <center>
        <img src="media/figures/sim2real.png" width="100%" class="interpolation-image"
         alt="Interpolate start reference image." />
            </center>
        </br>
          </br>
        <!-- Interpolating. -->
                <h3 class="title is-4">Comparisn with Baseline</h3>
        <div class="content has-text-justified">
          </div>

<div class="columns is-centered">
      <div class="column">

        <div class="content">

          <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
            <source src="media/videos/real_baseline.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>



      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
              <source src="media/videos/real_our.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
              </div>
               </div>
                    </div>
         </div>
        <br/>
        <br/>
              <br/>
        <br/>
         <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Compositional Generalization</h2>
        <p>
        We train a policy to pack objects of different shapes in the brown box, and put blocks of different colors in the bowls of different colors, and adapt it to put objects of different shapes in the bowls of different colors.
          </p>
          </br>
        <center>
        <img src="media/figures/compositional.png" width="76%" class="interpolation-image"
         alt="Interpolate start reference image." />
            </center>
        </br>
          </br>
        <!-- Interpolating. -->
        <h3 class="title is-4">Recorded Demonstration</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
                 <center>
            <video autoplay muted loop playsinline width="40%" style="border-radius: 10px">
              <source src="media/videos/record_shape_bowl.mp4"
                      type="video/mp4">
            </video>
          </center>
        <br/>
        <br/>
                <h3 class="title is-4">Comparisn with Baseline</h3>
        <div class="content has-text-justified">
          </div>

<div class="columns is-centered">
      <div class="column">

        <div class="content">

          <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
            <source src="media/videos/shape_bowl_baseline.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>



      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
              <source src="media/videos/shape_bowl_our.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
              </div>
               </div>
                    </div>
         </div>
        <br/>
        <br/>
              <br/>
        <br/>
         <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Out-of-distribution (Unseen Objects)</h2>
        <p>
        We train a policy to pack certain objects in the brown box, and adapt it to pack unseen objects.
          </p>
          </br>
        <center>
        <img src="media/figures/objects.png" width="54%" class="interpolation-image"
         alt="Interpolate start reference image." />
            </center>
        </br>
          </br>
        <!-- Interpolating. -->
        <h3 class="title is-4">Recorded Demonstration</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
                 <center>
            <video autoplay muted loop playsinline width="40%" style="border-radius: 10px">
              <source src="media/videos/record_objects.mp4"
                      type="video/mp4">
            </video>
          </center>
        <br/>
        <br/>
                <h3 class="title is-4">Comparisn with Baseline</h3>
        <div class="content has-text-justified">
          </div>

<div class="columns is-centered">
      <div class="column">

        <div class="content">

          <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
            <source src="media/videos/objects_baseline.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>



      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
              <source src="media/videos/objects_our.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
              </div>
               </div>
                    </div>
         </div>
        <br/>
        <br/>
              <br/>
        <br/>
         <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Out-of-distribution (Unseen Environment)</h2>
        <p>
        We train a policy on seen environments and adapt it to a new environment with different textures and differently positioned static elements such as the sliding door, the drawer and the light button.
          </p>
          </br>
        <center>
        <img src="media/figures/environment.png" width="74%" class="interpolation-image"
         alt="Interpolate start reference image." />
            </center>
        </br>
          </br>
        <!-- Interpolating. -->
        <h3 class="title is-4">Recorded Demonstration</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
                 <center>
            <video autoplay muted loop playsinline width="40%" style="border-radius: 10px">
              <source src="media/videos/record_calvin.mp4"
                      type="video/mp4">
            </video>
          </center>
        <br/>
        <br/>
                <h3 class="title is-4">Comparisn with Baseline</h3>
        <div class="content has-text-justified">
          </div>

<div class="columns is-centered">
      <div class="column">

        <div class="content">

          <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
            <source src="media/videos/calvin_baseline.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>



      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video autoplay muted loop playsinline width="80%" style="border-radius: 10px">
              <source src="media/videos/calvin_our.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
              </div>
               </div>
                    </div>
         </div>

    </div>
         </div>
        </section>





<!--
<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2022peract,
  title     = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation}, 
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year      = {2022},
}</code></pre>
  </div>
</section>
-->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
