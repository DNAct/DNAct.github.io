
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Diffusion-Guided 3D Policy Learning for Generalizable Robotic Manipulation">
  <meta name="keywords" content="Neural Rendering, Robotic Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NerFuser</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>
    function updateSingleVideo() {
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "media/results/sim_rollouts/" + 
                  "compressed_" +
                  task +
                  "-" +
                  inst +
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Diffusion-Guided 3D Policy Learning for Generalizable Robotic Manipulation</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank">Anonymous</a></h3>
<!--           <div class="is-size-5 publication-authors">
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Artificial Intelligence Laboratory,</span>
            <span class="author-block"><sup>3</sup>Shanghai Qi Zhi Institute</span>
          </div>

          <div class="is-size-6 publication-authors"> *Equal Contribution </div> -->

<!--           <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/pdf/2306.10474.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
<!--               <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2306.10474"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="https://youtu.be/UdzoagBgWTA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://semantic-geometric-representation.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <img src="/figures/main.png" alt="Description of the image">
      <h2 class="subtitle has-text-centered">
      </br>
        Leveraging semantic information from massive 2D images and geometric information from 
        3D point clouds, we present Semantic-Geometric Representation (SGR) that enables the robots 
        to solve a range of simulated and real-world manipulation tasks.
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper presents NeRFuser, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. 
		  Leveraging feature distilled from foundation models such as Stable Diffusion, we learn a generalizable 3D representation by pre-training a 3D voxel encoder via neural rendering. 
		  Consequently, it allows various application to challenging robotic tasks requiring rich 3D semantics and accurate geometry. 
		  Furthermore, we introduce a novel approach utilizing diffusion training to optimize features fused with vision and language embeddings. 
		  By concentrating on the optimization of fused features and predict actions with an additional policy network, we not only eliminate the need for time-consuming denoising process during action inference but also align features with the multi-modality inherent in multi-task demonstrations, thereby enhancing robustness and generalizability. 
		  NeRFuser significantly surpasses SOTA NeRF- based multi-task manipulation approaches, achieving a 1.37x increase in success rate.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">Semantic-Geometric Representation</span></h2>

        <img src="media/figures/architecture.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        </br>
          <p>
            We first utilize a large vision foundation model, pre-trained on massive amounts 
            of internet data (e.g., CLIP), to encode semantic feature maps from 2D images. 
            Secondly, the context-rich 2D feature vectors are back-projected into 3D space 
            and combined with the point cloud features that are extracted from point clouds 
            using a shallow point-based network. These fused features are fed into a number 
            of set abstraction (SA) blocks, which jointly model the cross-modal interaction 
            between 2D semantics and 3D geometry information. Finally, based on the output 
            representations from the SA blocks, we predict the robotic action to execute.
          </p>

      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Real-Robot Results</h2>

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h3 class="title is-5">Picking Red Block</h3>
          <video id="dollyzoom" autoplay muted loop playsinline height="30%">
            <source src="/videos/Stack_block.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-5">Opening Drawer</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="dollyzoom" autoplay muted loop playsinline height="30%">
              <source src="/videos/sweep_dustpan.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h3 class="title is-5">Hitting Ball</h3>
          <video id="dollyzoom" autoplay muted loop playsinline height="30%">
            <source src="/videos/hit_ball.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-5">Putting in Bowl</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="dollyzoom" autoplay muted loop playsinline height="30%">
              <source src="/videos/Put_in_bowl.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h3 class="title is-5">Moving Cup to Goal</h3>
          <video id="dollyzoom" autoplay muted loop playsinline height="30%">
            <source src="/videos/put_in_bin.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

<!--       Matting. -->
      <div class="column">
        <h2 class="title is-5">Putting Banana in Pot</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="dollyzoom" autoplay muted loop playsinline height="100%">
              <source src="./media/real_robot/banana.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h3 class="title is-5">Pressing Handsan</h3>
          <video id="dollyzoom" autoplay muted loop playsinline height="100%">
            <source src="./media/real_robot/handsan.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-5">Putting Marker in Drawer</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="dollyzoom" autoplay muted loop playsinline height="100%">
              <source src="./media/real_robot/pen.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->
  </div>
</section>
 -->
	
<section class="section">
  <div class="container is-max-desktop">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Simulation Results</h2>

        <div class="columns">
          <div class="column has-text-centered">
		  
            One multi-task agent equipped with SGR, evaluated on 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="open_microwave" selected="selected">open microwave</option>
              <option value="open_door">open door</option>
              <option value="water_plants">water plants</option>
              <option value="toilet_seat_up">toilet seat up</option>
              <option value="phone_on_base">phone on base</option>
              <option value="put_books_on_bookshelf">put books</option>
              <option value="take_umbrella_out_of_umbrella_stand">take out umbrella</option>
              <option value="open_fridge">open fridge</option>
              </select>
            </div>
            episode
            <div class="select is-small">
              <select id="single-menu-instances" onchange="updateSingleVideo()">
              <option value="s1">01</option>
              <option value="s2" selected="selected">02</option>
              <option value="s3">03</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="multi-task-result-video"
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="media/results/sim_rollouts/compressed_open_microwave-s2.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2023universal,
      title={A Universal Semantic-Geometric Representation for Robotic Manipulation},
      author={Zhang, Tong and Hu, Yingdong and Cui, Hanchen and Zhao, Hang and Gao, Yang},
      journal={arXiv preprint arXiv:2306.10474},
      year={2023}
    }</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://github.com/cliport/cliport.github.io">CLIPort</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
